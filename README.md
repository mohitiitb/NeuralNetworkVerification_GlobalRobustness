## Verification of Neural Networks: Specifying Global Robustness using Generative Models

We present experiments exploring the notions of global correctness and global robustness defined in our research paper:

[NathanaÃ«l Fijalkow](https://nathanael-fijalkow.github.io/) and Mohit Kumar Gupta
**Verification of Neural Networks: Specifying Global Robustness using Generative Models**
[arXiv link](TBA)

The experiments are in Jupter notebook format:
* [Random walk](https://github.com/mohitiitb/NeuralNetworkVerification_GlobalRobustness/blob/master/Random_walk.ipynb)
* [Analysis of an image classifier using a generative model](https://github.com/mohitiitb/NeuralNetworkVerification_GlobalRobustness/blob/master/Analysis.ipynb)
* [Evaluating the global correctness](https://github.com/mohitiitb/NeuralNetworkVerification_GlobalRobustness/blob/master/Global_correctness.ipynb)
* [Searching for Realistic Adversarial Examples: black-box approach](https://github.com/mohitiitb/NeuralNetworkVerification_GlobalRobustness/blob/master/Adversarial%20Examples%20Black-box.ipynb)
* [Searching for Realistic Adversarial Examples: white-box approach](https://github.com/mohitiitb/NeuralNetworkVerification_GlobalRobustness/blob/master/Adversarial%20Examples%20White-box.ipynb)
* [Dependence on the generative model: disjoint training sets](https://github.com/mohitiitb/NeuralNetworkVerification_GlobalRobustness/blob/master/EMNIST.ipynb)

All experiments use Tensorflow, and pre-trained models can be used (see /Models).
